{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266f3dee-6ee0-4f6d-9b18-c6092165cc85",
   "metadata": {},
   "source": [
    "### 1. Data Ingestion Pipeline:\n",
    "\n",
    "#### a. Design a data ingestion pipeline that collects and stores data from various sources such as databases, APIs, and streaming platforms.\n",
    "\n",
    "To design a data ingestion pipeline in Python that collects and stores data from various sources, such as databases, APIs, and streaming platforms, you can follow these general steps:\n",
    "\n",
    "1. Identify Data Sources:\n",
    "\n",
    "Determine the data sources you want to collect from, such as databases, APIs, or streaming platforms.\n",
    "\n",
    "Understand the data formats, access methods, and any authentication or authorization requirements for each source.\n",
    "\n",
    "2. Choose Data Collection Tools and Libraries:\n",
    "\n",
    "Select appropriate Python libraries and tools that can interact with the different data sources.\n",
    "\n",
    "For databases, you can use libraries like SQLAlchemy, psycopg2, or pymongo, depending on the database type.\n",
    "\n",
    "For APIs, libraries like requests or specialized API wrappers can be used.\n",
    "\n",
    "For streaming platforms, libraries like Kafka-Python, Pulsar, or Apache Beam can be utilized.\n",
    "\n",
    "3. Establish Connection and Authentication:\n",
    "\n",
    "Set up connections to the data sources using appropriate connection parameters or credentials.\n",
    "\n",
    "Configure authentication methods, such as API keys or OAuth tokens, as required by the data sources.\n",
    "\n",
    "4. Fetch and Collect Data:\n",
    "\n",
    "Write functions or classes to fetch data from each source, using the corresponding libraries.\n",
    "\n",
    "For databases, you can write SQL queries or use ORM (Object-Relational Mapping) techniques.\n",
    "\n",
    "For APIs, make HTTP requests and process the responses to extract the required data.\n",
    "\n",
    "For streaming platforms, set up consumer or subscriber clients to consume the data stream.\n",
    "\n",
    "5. Handle Data Transformations and Preprocessing:\n",
    "\n",
    "Perform any necessary data transformations or preprocessing steps to clean, format, or enrich the collected data.\n",
    "\n",
    "Use appropriate libraries for data manipulation, cleaning, and transformation, such as pandas or NumPy.\n",
    "\n",
    "6. Define Storage Mechanisms:\n",
    "\n",
    "Determine the storage mechanisms based on your requirements, such as databases, data lakes, or file systems.\n",
    "\n",
    "Choose suitable storage technologies like PostgreSQL, MySQL, MongoDB, Apache Hadoop, Apache Parquet, or Amazon S3.\n",
    "\n",
    "7. Write Data to Storage:\n",
    "\n",
    "Develop code to write the collected and processed data to the chosen storage mechanisms.\n",
    "\n",
    "Utilize appropriate libraries or database connectors to insert or write the data.\n",
    "\n",
    "Ensure data integrity, consistency, and error handling during the writing process.\n",
    "\n",
    "8. Implement Scheduling and Automation:\n",
    "\n",
    "Set up scheduling mechanisms, such as cron jobs or task schedulers, to automate the data ingestion pipeline.\n",
    "\n",
    "Determine the frequency of data collection and define the intervals or triggers accordingly.\n",
    "\n",
    "9. Implement Error Handling and Logging:\n",
    "\n",
    "Include error handling mechanisms to handle exceptions or failures during data collection or storage.\n",
    "\n",
    "Use logging frameworks, such as Python's built-in logging module or third-party libraries like loguru or structlog, to log pipeline activities, errors, and information.\n",
    "\n",
    "10. Monitor and Maintain:\n",
    "\n",
    "Monitor the data ingestion pipeline for performance, data quality, and any potential issues.\n",
    "\n",
    "Implement monitoring and alerting mechanisms to identify and address any pipeline failures or anomalies.\n",
    "\n",
    "Regularly review and maintain the pipeline to adapt to changes in data sources or requirements.\n",
    "\n",
    "Remember, the specific implementation details and libraries used may vary depending on the exact data sources, storage mechanisms, and requirements of your data ingestion pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de1ca86-3000-4077-a0f6-20edd77e1a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Downloading psycopg2-2.9.6.tar.gz (383 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.0/384.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[25 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /opt/conda/lib/python3.10/site-packages/setuptools/config/setupcfg.py:508: SetuptoolsDeprecationWarning: The license_file parameter is deprecated, use license_files instead.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(msg, warning_class)\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m creating /tmp/pip-pip-egg-info-bl4y7_z9/psycopg2.egg-info\n",
      "  \u001b[31m   \u001b[0m writing /tmp/pip-pip-egg-info-bl4y7_z9/psycopg2.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to /tmp/pip-pip-egg-info-bl4y7_z9/psycopg2.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to /tmp/pip-pip-egg-info-bl4y7_z9/psycopg2.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m writing manifest file '/tmp/pip-pip-egg-info-bl4y7_z9/psycopg2.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Error: pg_config executable not found.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m pg_config is required to build psycopg2 from source.  Please add the directory\n",
      "  \u001b[31m   \u001b[0m containing pg_config to the $PATH or specify the full executable path with the\n",
      "  \u001b[31m   \u001b[0m option:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     python setup.py build_ext --pg-config /path/to/pg_config build ...\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m or with the pg_config option in 'setup.cfg'.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you prefer to avoid building psycopg2 from source, please install the PyPI\n",
      "  \u001b[31m   \u001b[0m 'psycopg2-binary' package instead.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m For further information please check the 'doc/src/install.rst' file (also at\n",
      "  \u001b[31m   \u001b[0m <https://www.psycopg.org/docs/install.html>).\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d5016ed-15d4-4ac7-880b-f0b197205240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n",
      "  Downloading pymongo-4.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (603 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m603.6/603.6 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dnspython<3.0.0,>=1.16.0\n",
      "  Downloading dnspython-2.3.0-py3-none-any.whl (283 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
      "Successfully installed dnspython-2.3.0 pymongo-4.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04da5c0-d185-447d-818a-c71eed2ac8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import pymongo\n",
    "from kafka import KafkaConsumer\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the database\n",
    "db_conn = psycopg2.connect(database=\"your_database\", user=\"your_username\", password=\"your_password\", host=\"localhost\", port=\"5432\")\n",
    "db_cursor = db_conn.cursor()\n",
    "\n",
    "# Connect to MongoDB\n",
    "mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017\")\n",
    "mongo_db = mongo_client[\"your_database\"]\n",
    "mongo_collection = mongo_db[\"your_collection\"]\n",
    "\n",
    "# Create an engine for SQLAlchemy\n",
    "db_engine = create_engine('postgresql://your_username:your_password@localhost:5432/your_database')\n",
    "\n",
    "# Connect to Kafka\n",
    "consumer = KafkaConsumer('your_topic', bootstrap_servers=['localhost:9092'])\n",
    "\n",
    "# Fetch data from API\n",
    "response = requests.get('your_api_url')\n",
    "api_data = json.loads(response.text)\n",
    "\n",
    "# Process and store the API data\n",
    "processed_api_data = process_api_data(api_data)\n",
    "db_cursor.execute(\"INSERT INTO your_table (column1, column2) VALUES (%s, %s)\", (processed_api_data['value1'], processed_api_data['value2']))\n",
    "db_conn.commit()\n",
    "\n",
    "# Fetch and process data from Kafka\n",
    "for message in consumer:\n",
    "    kafka_data = json.loads(message.value)\n",
    "    processed_kafka_data = process_kafka_data(kafka_data)\n",
    "    mongo_collection.insert_one(processed_kafka_data)\n",
    "\n",
    "# Fetch data from a database table\n",
    "query = \"SELECT * FROM your_table\"\n",
    "df = pd.read_sql_query(query, db_engine)\n",
    "\n",
    "# Perform data transformations and preprocessing\n",
    "transformed_data = transform_data(df)\n",
    "\n",
    "# Store the transformed data in a file\n",
    "transformed_data.to_csv('transformed_data.csv', index=False)\n",
    "\n",
    "# Close database connections and Kafka consumer\n",
    "db_cursor.close()\n",
    "db_conn.close()\n",
    "mongo_client.close()\n",
    "consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaca5eef-0c75-4c69-81e7-b45db7cd8067",
   "metadata": {},
   "source": [
    "### Implement a real-time data ingestion pipeline for processing sensor data from IoT devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ea458-eafc-468a-8181-6561519e2506",
   "metadata": {},
   "source": [
    "To implement a real-time data ingestion pipeline for processing sensor data from IoT devices in Python, you can utilize a combination of technologies such as MQTT (Message Queuing Telemetry Transport) protocol, MQTT broker, and a Python MQTT client library. Here's an example code snippet that demonstrates the basic implementation of such a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f813c964-b209-4a79-ae68-9d133f425f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paho.mqtt.client as mqtt\n",
    "import json\n",
    "import time\n",
    "\n",
    "# MQTT broker settings\n",
    "broker_address = \"mqtt_broker_address\"\n",
    "broker_port = 1883\n",
    "topic = \"your_topic\"\n",
    "\n",
    "# Define callback functions for MQTT events\n",
    "def on_connect(client, userdata, flags, rc):\n",
    "    print(\"Connected to MQTT broker with result code: \" + str(rc))\n",
    "    client.subscribe(topic)\n",
    "\n",
    "def on_message(client, userdata, msg):\n",
    "    payload = msg.payload.decode(\"utf-8\")\n",
    "    data = json.loads(payload)\n",
    "\n",
    "    # Process and analyze the received sensor data\n",
    "    process_sensor_data(data)\n",
    "\n",
    "# Create an MQTT client instance\n",
    "client = mqtt.Client()\n",
    "\n",
    "# Set MQTT event callbacks\n",
    "client.on_connect = on_connect\n",
    "client.on_message = on_message\n",
    "\n",
    "# Connect to the MQTT broker\n",
    "client.connect(broker_address, broker_port, 60)\n",
    "\n",
    "# Start the MQTT client loop to handle incoming messages\n",
    "client.loop_start()\n",
    "\n",
    "# Continuously process sensor data until interrupted\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted, stopping data ingestion.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45854ad-baea-470f-a472-50921f6f2700",
   "metadata": {},
   "source": [
    "# Disconnect from the MQTT broker\n",
    "client.disconnect()\n",
    "In the above code snippet, we're using the Paho MQTT client library (paho.mqtt.client) to connect to an MQTT broker, subscribe to a specific topic, and receive real-time sensor data messages from IoT devices. The on_connect and on_message callback functions handle the corresponding MQTT events. The on_message function processes and analyzes the received sensor data using the process_sensor_data function (which you can define as per your requirements).\n",
    "\n",
    "To utilize this code, you need to replace \"mqtt_broker_address\" with the actual address of your MQTT broker, update the \"your_topic\" placeholder with the desired topic to subscribe to, and implement the process_sensor_data function to handle the received data.\n",
    "\n",
    "Remember to install the paho-mqtt library before running the code. You can install it via pip using the command: pip install paho-mqtt.\n",
    "\n",
    "Additionally, you may need to handle authentication, encryption, and other security aspects based on the MQTT broker configuration and requirements of your IoT infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fbdb4e-a5f2-488d-8716-3e57b64dd731",
   "metadata": {},
   "source": [
    "### c. Develop a data ingestion pipeline that handles data from different file formats (CSV, JSON, etc.) and performs data validation and cleansing.\n",
    "\n",
    "_ To develop a data ingestion pipeline in Python that handles data from different file formats (such as CSV, JSON) and performs data validation and cleansing, you can utilize libraries like pandas and json. Here's an example code snippet that demonstrates the basic implementation of such a pipeline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33471cb-572a-459c-9364-c04c3ae1be88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Function to read and process CSV files\n",
    "def process_csv_file(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Perform data validation and cleansing operations on the DataFrame\n",
    "        # ...\n",
    "\n",
    "        # Save the cleaned data to a new file or perform further processing\n",
    "        df.to_csv('cleaned_data.csv', index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing CSV file: {str(e)}\")\n",
    "\n",
    "# Function to read and process JSON files\n",
    "def process_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path) as json_file:\n",
    "            data = json.load(json_file)\n",
    "\n",
    "        # Perform data validation and cleansing operations on the JSON data\n",
    "        # ...\n",
    "\n",
    "        # Save the cleaned data to a new file or perform further processing\n",
    "        with open('cleaned_data.json', 'w') as json_output:\n",
    "            json.dump(data, json_output)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing JSON file: {str(e)}\")\n",
    "\n",
    "# File paths for example CSV and JSON files\n",
    "csv_file_path = 'example.csv'\n",
    "json_file_path = 'example.json'\n",
    "\n",
    "# Process CSV file\n",
    "process_csv_file(csv_file_path)\n",
    "\n",
    "# Process JSON file\n",
    "process_json_file(json_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dff2c27-5014-4e41-b42b-f1fcae18c696",
   "metadata": {},
   "source": [
    "In the above code snippet, we define two functions, process_csv_file and process_json_file, to handle CSV and JSON files, respectively. These functions use pandas and json libraries to read the files and perform data validation and cleansing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f154c-9a95-4517-ad9b-e27a645ab546",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
